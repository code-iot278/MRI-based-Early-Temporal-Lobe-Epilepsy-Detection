# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i9jHOxO_wNCOe_EmLDev_51GBsBxGjyR
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **library**"""

pip install SimpleITK

!pip install scikit-fuzzy

"""# **Nii to Image convertion**"""

pip install nibabel numpy pillow

import nibabel as nib
import numpy as np
import os
from PIL import Image

# Input main folder containing subfolders with NIfTI files
main_input_folder = "/content/drive/MyDrive/Colab Notebooks/UNAM/UNAM Dataset"

# Output main folder for images
output_main_folder = "/content/drive/MyDrive/Colab Notebooks/UNAM/output_images"
os.makedirs(output_main_folder, exist_ok=True)

# Loop through each subfolder in the main input folder
for subfolder in os.listdir(main_input_folder):
    subfolder_path = os.path.join(main_input_folder, subfolder)

    if os.path.isdir(subfolder_path):  # Ensure it's a folder
        output_folder = os.path.join(output_main_folder, subfolder)  # Corresponding output subfolder
        os.makedirs(output_folder, exist_ok=True)

        # Process each NIfTI file in the subfolder
        for file in os.listdir(subfolder_path):
            if file.endswith(".nii") or file.endswith(".nii.gz"):  # Ensure it's a NIfTI file
                nii_file = os.path.join(subfolder_path, file)

                # Load the NIfTI file
                nii_data = nib.load(nii_file)
                image_array = nii_data.get_fdata()

                # Ensure the array is 3D (x, y, z) or 4D (x, y, z, t)
                if len(image_array.shape) == 4:
                    image_array = image_array[:, :, :, 0]  # Take the first time frame if 4D

                # Normalize pixel values to 0-255
                image_array = (image_array - np.min(image_array)) / (np.max(image_array) - np.min(image_array)) * 255
                image_array = image_array.astype(np.uint8)

                # Select a middle slice along the Z-axis
                slice_index = image_array.shape[2] // 2
                image_slice = image_array[:, :, slice_index]

                # Ensure the extracted slice is 2D
                if image_slice.ndim == 3:
                    image_slice = image_slice[:, :, 0]  # Convert multi-channel to grayscale

                # Convert to PIL image
                image = Image.fromarray(image_slice, mode='L')  # 'L' for grayscale images

                # Save as PNG
                output_image_path = os.path.join(output_folder, f"{os.path.splitext(file)[0]}.png")
                image.save(output_image_path)
                print(f"Image saved at: {output_image_path}")

"""# **Pre-processing**
# **Skull Stripping**
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Path to the main folder containing subfolders with MRI images
main_folder_path = '/content/drive/MyDrive/Colab Notebooks/UNAM/output_images'

# Output folder for the processed images
output_folder = '/content/drive/MyDrive/Colab Notebooks/UNAM/Healthy_Skull_Stripped'

# Create the output folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)

# Recursively walk through all subfolders and files
for root, dirs, files in os.walk(main_folder_path):
    # Loop through each file in the current folder
    for filename in files:
        # Create the full path to the image
        image_path = os.path.join(root, filename)

        # Create corresponding subfolder structure in the output folder
        relative_path = os.path.relpath(root, main_folder_path)
        output_subfolder = os.path.join(output_folder, relative_path)
        os.makedirs(output_subfolder, exist_ok=True)

        # Print the image being processed
        print(f"Processing image: {filename}")

        # Load the MRI image
        mri_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        # Check if the image is loaded successfully
        if mri_image is None:
            print(f"Error loading image: {filename}")
            continue  # Skip to the next iteration

        # Apply Gaussian blur to the image
        blurred_image = cv2.GaussianBlur(mri_image, (15, 15), 0)

        # Perform Otsu's thresholding
        _, thresholded_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # Invert the thresholded image to get the skull mask
        skull_mask = cv2.bitwise_not(thresholded_image)

        # Apply morphological operations to remove the outer shell of the skull
        kernel = np.ones((10, 10), np.uint8)
        inner_brain_mask = cv2.morphologyEx(skull_mask, cv2.MORPH_CLOSE, kernel)

        # Create an image with the removed part (original - inner_brain)
        removed_part_image = cv2.subtract(mri_image, inner_brain_mask)

        # Display the images
        plt.figure(figsize=(15, 5))

        plt.subplot(1, 3, 1)
        plt.imshow(mri_image, cmap='gray')
        plt.title('Original Image')

        plt.subplot(1, 3, 2)
        plt.imshow(inner_brain_mask, cmap='gray')
        plt.title('Inner Brain Mask')

        plt.subplot(1, 3, 3)
        plt.imshow(removed_part_image, cmap='gray')
        plt.title('Removed Outer Shell of Skull')

        plt.show()

        # Save the removed outer shell of the skull to the corresponding output subfolder
        output_image_path = os.path.join(output_subfolder, filename)
        cv2.imwrite(output_image_path, removed_part_image)

        # Display a message indicating the processing of the current image
        print(f"Processed: {filename}")

print("Skull stripping and saving completed.")

"""# **Bias Field Correction**"""

import os
import SimpleITK as sitk
import matplotlib.pyplot as plt

def bias_field_correction(input_image_path, output_image_path):
    # Read the input image
    input_image = sitk.ReadImage(input_image_path, sitk.sitkFloat32)

    # Initialize the N4 Bias Field Correction filter
    n4_filter = sitk.N4BiasFieldCorrectionImageFilter()

    # Apply the N4 bias field correction
    corrected_image = n4_filter.Execute(input_image)

    # Convert the corrected image to an 8-bit unsigned integer format
    corrected_image_uint8 = sitk.Cast(corrected_image, sitk.sitkUInt8)

    # Save the corrected image
    sitk.WriteImage(corrected_image_uint8, output_image_path)

    # Display the input and output images
    display_images(input_image, corrected_image)

def display_images(input_image, corrected_image):
    """
    Display input and corrected images side by side.

    Args:
        input_image (SimpleITK.Image): Original input image.
        corrected_image (SimpleITK.Image): Corrected output image.
    """
    # Convert SimpleITK images to numpy arrays
    input_array = sitk.GetArrayFromImage(input_image)
    corrected_array = sitk.GetArrayFromImage(corrected_image)

    # Check if the images are 3D and take the middle slice
    if input_array.ndim == 3:
        mid_slice_index = input_array.shape[0] // 2
        input_display = input_array[mid_slice_index, :, :]
        corrected_display = corrected_array[mid_slice_index, :, :]
    else:
        input_display = input_array
        corrected_display = corrected_array

    # Plot the images
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.title("Original Image")
    plt.imshow(input_display, cmap='gray')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.title("Corrected Image")
    plt.imshow(corrected_display, cmap='gray')
    plt.axis('off')

    plt.show()

def process_images_in_folder(input_folder, output_folder):
    # Create the output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Walk through the input folder and its subdirectories
    for root, _, files in os.walk(input_folder):
        for filename in files:
            if filename.endswith('.png'):  # Change this if you have different image formats
                input_image_path = os.path.join(root, filename)

                # Generate the output path while maintaining the subfolder structure
                relative_path = os.path.relpath(root, input_folder)
                output_subfolder = os.path.join(output_folder, relative_path)
                os.makedirs(output_subfolder, exist_ok=True)

                output_image_path = os.path.join(output_subfolder, f"corrected_{filename}")

                print(f"Processing: {input_image_path}")
                bias_field_correction(input_image_path, output_image_path)

# Example usage
input_folder = '/content/drive/MyDrive/Colab Notebooks/UNAM/Healthy_Skull_Stripped'  # Change this to your input folder path
output_folder = '/content/drive/MyDrive/Colab Notebooks/UNAM/output_images4'  # Change this to your desired output folder path

process_images_in_folder(input_folder, output_folder)

"""# **Normalization min-max**"""

import numpy as np
import SimpleITK as sitk
import os
import matplotlib.pyplot as plt

def min_max_normalization(image):
    """
    Perform Min-Max normalization on a SimpleITK image.

    Args:
        image (SimpleITK.Image): Input image to be normalized.

    Returns:
        SimpleITK.Image: Min-Max normalized image.
    """
    # Convert the image to a numpy array
    image_array = sitk.GetArrayFromImage(image)

    # Compute the min and max values
    min_val = np.min(image_array)
    max_val = np.max(image_array)

    # Apply Min-Max normalization
    normalized_array = (image_array - min_val) / (max_val - min_val) * 255
    normalized_array = normalized_array.astype(np.uint8)

    # Convert back to a SimpleITK image
    normalized_image = sitk.GetImageFromArray(normalized_array)
    normalized_image.CopyInformation(image)  # Copy metadata

    return normalized_image

def display_images(original_image, normalized_image):
    """
    Display original and normalized images side by side.

    Args:
        original_image (SimpleITK.Image): Original input image.
        normalized_image (SimpleITK.Image): Normalized output image.
    """
    original_array = sitk.GetArrayFromImage(original_image)
    normalized_array = sitk.GetArrayFromImage(normalized_image)

    # Take the middle slice for 3D images or the entire image for 2D images
    if original_array.ndim == 3:
        mid_slice_index = original_array.shape[0] // 2
        original_display = original_array[mid_slice_index, :, :]
        normalized_display = normalized_array[mid_slice_index, :, :]
    else:
        original_display = original_array
        normalized_display = normalized_array

    # Plot the images
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.title("Original Image")
    plt.imshow(original_display, cmap='gray')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.title("Min-Max Normalized Image")
    plt.imshow(normalized_display, cmap='gray')
    plt.axis('off')

    plt.show()

def process_images_in_folder(input_folder, output_folder):
    """
    Process all images in the input folder and its subfolders, applying Min-Max normalization.

    Args:
        input_folder (str): Path to the input folder containing images and subfolders.
        output_folder (str): Path to the output folder for saving normalized images.
    """
    # Create the output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Walk through the input folder and its subdirectories
    for root, _, files in os.walk(input_folder):
        for filename in files:
            if filename.endswith('.png'):  # Change this if you have different image formats
                input_image_path = os.path.join(root, filename)

                # Generate the output path while maintaining the subfolder structure
                relative_path = os.path.relpath(root, input_folder)
                output_subfolder = os.path.join(output_folder, relative_path)
                os.makedirs(output_subfolder, exist_ok=True)

                output_image_path = os.path.join(output_subfolder, f"normalized_{filename}")

                print(f"Processing: {input_image_path}")

                # Read the input image
                input_image = sitk.ReadImage(input_image_path, sitk.sitkFloat32)

                # Apply Min-Max normalization
                normalized_image = min_max_normalization(input_image)

                # Save the normalized image
                sitk.WriteImage(normalized_image, output_image_path)

                # Display the original and normalized images
                display_images(input_image, normalized_image)

# Example usage
input_folder = '/content/drive/MyDrive/Colab Notebooks/UNAM/output_images4'  # Change this to your input folder path
output_folder = '/content/drive/MyDrive/Colab Notebooks/UNAM/normalization_images'  # Change this to your desired output folder path

process_images_in_folder(input_folder, output_folder)

"""# **Noise Reduction- Median filter**"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt

def apply_median_filter(image, kernel_size=1):
    """
    Apply Median filter to the input image.

    Args:
        image (numpy.ndarray): Input image.
        kernel_size (int): Kernel size for the Median filter.

    Returns:
        numpy.ndarray: Filtered image.
    """
    filtered_image = cv2.medianBlur(image, kernel_size)
    return filtered_image

def display_images(original_image, filtered_image):
    """
    Display original and filtered images side by side.

    Args:
        original_image (numpy.ndarray): Original image.
        filtered_image (numpy.ndarray): Filtered image.
    """
    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
    filtered_image_rgb = cv2.cvtColor(filtered_image, cv2.COLOR_BGR2RGB)

    # Display the images
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.title("Original Image")
    plt.imshow(original_image_rgb)
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.title("Filtered Image (Median)")
    plt.imshow(filtered_image_rgb)
    plt.axis('off')

    plt.tight_layout()
    plt.show()

def process_images_in_folders(input_folder_path, output_folder_path):
    """
    Process all images in the input folder and its subfolders by applying a Median filter.

    Args:
        input_folder_path (str): Path to the main input folder containing images and subfolders.
        output_folder_path (str): Path to the output folder for saving filtered images.
    """
    # Create the output folder if it doesn't exist
    os.makedirs(output_folder_path, exist_ok=True)

    # Traverse the directory tree, including subfolders
    for root, _, files in os.walk(input_folder_path):
        for filename in files:
            if filename.endswith('.png') or filename.endswith('.jpg'):  # Image formats to process
                image_path = os.path.join(root, filename)

                # Read the image
                image = cv2.imread(image_path, cv2.IMREAD_COLOR)

                if image is not None:
                    # Apply Median filter
                    filtered_image = apply_median_filter(image)

                    # Display the original and filtered images
                    display_images(image, filtered_image)

                    # Preserve folder structure in the output folder
                    relative_path = os.path.relpath(root, input_folder_path)
                    output_subfolder = os.path.join(output_folder_path, relative_path)
                    os.makedirs(output_subfolder, exist_ok=True)

                    # Save the filtered image to the corresponding subfolder in the output folder
                    output_image_path = os.path.join(output_subfolder, filename)
                    cv2.imwrite(output_image_path, filtered_image)
                    print(f"Saved filtered image: {output_image_path}")
                else:
                    print(f"Error loading image: {filename}")

# Example usage
input_folder_path = '/content/drive/MyDrive/Colab Notebooks/UNAM/normalization_images'  # Main input folder path
output_folder_path = '/content/drive/MyDrive/Colab Notebooks/UNAM/filtered_images'  # Main output folder path

process_images_in_folders(input_folder_path, output_folder_path)

print("Processing and saving completed.")

"""# **Segmentation phase via Fuzzy-AAL Segmentation Framework (FASF)**
# **Fuzzy Possibilistic C-Means (FPCM)**
"""

import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
import skfuzzy as fuzz
from skimage.filters import median
from skimage.morphology import disk
from scipy.ndimage import affine_transform
import random
def load_mri_image(file_path):
    img = nib.load(file_path)
    return img.get_fdata()

def apply_median_filter(image_slice):
    return median(image_slice, disk(3))

def fpcm_segmentation(image_slice, n_clusters=3, m=2, eta=2):
    image_flat = image_slice.flatten()
    cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(
        np.expand_dims(image_flat, axis=0), n_clusters, m, error=0.005, maxiter=1000, init=None
    )
    t = np.sum((u ** eta) * image_flat, axis=1) / np.sum(u ** eta, axis=1)
    p = np.exp(-np.abs(image_flat[:, None] - t) / np.std(image_flat))
    segmentation_result = np.argmax(u + p.T, axis=0)
    return segmentation_result.reshape(image_slice.shape)
def segment_image_fpcm(img, n_clusters=3):

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    pixel_values = gray.reshape((-1, 1)).astype(np.float64)

    # Apply FPCM clustering
    cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(
        pixel_values.T, n_clusters, 2, error=0.005, maxiter=1000, init=None
    )

    # Assign cluster labels
    labels = np.argmax(u, axis=0)
    segmented_img = labels.reshape(gray.shape)

    return segmented_img

def load_aal_template(aal_path):
    aal_img = nib.load(aal_path)
    return aal_img.get_fdata()

def align_to_aal(segmented_mri, aal_template, transformation_matrix):
    return affine_transform(segmented_mri, transformation_matrix)

def map_to_aal_labels(segmented_mri, aal_template):
    aal_labels = np.zeros_like(segmented_mri)
    unique_regions = np.unique(aal_template)
    for region in unique_regions:
        mask = aal_template == region
        if np.any(mask):
            aal_labels[mask] = region
    return aal_labels
def random_fmri_activation(mri_img, num_regions=(1, 2)):
    """
    Generates a random-positioned fMRI activation overlay on an MRI slice.
    """
    color_mri = cv2.cvtColor(mri_img, cv2.COLOR_GRAY2RGB)
    height, width = mri_img.shape

    # Create activation map
    heatmap = np.zeros_like(mri_img, dtype=np.float32)

    # Choose a random number of activation regions (1 or 2)
    num_activations = random.randint(*num_regions)

    for _ in range(num_activations):
        # Choose random positions within the image
        activation_x = random.randint(int(0.2 * width), int(0.8 * width))
        activation_y = random.randint(int(0.2 * height), int(0.8 * height))
        activation_radius = random.randint(int(min(width, height) * 0.08), int(min(width, height) * 0.15))

        # Draw activation circle
        cv2.circle(heatmap, (activation_x, activation_y), activation_radius, 1, -1)

    # Apply Gaussian blur
    heatmap = cv2.GaussianBlur(heatmap, (15, 15), 8)

    # Normalize heatmap
    heatmap = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

    # Convert heatmap to red colormap
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_HOT)

    # Apply mask to keep activation inside brain region
    mask = cv2.threshold(mri_img, 10, 255, cv2.THRESH_BINARY)[1]
    masked_heatmap = cv2.bitwise_and(heatmap_color, heatmap_color, mask=mask)

    # Blend with MRI image
    output_img = cv2.addWeighted(color_mri, 0.6, masked_heatmap, 0.5, 0)

    return output_img

def full_segmentation_pipeline(mri_path, aal_path, transformation_matrix):
    mri_data = load_mri_image(mri_path)
    aal_template = load_aal_template(aal_path)
    segmented_slices = []
    for i in range(mri_data.shape[2]):
        slice_median = apply_median_filter(mri_data[:, :, i])
        segmented = fpcm_segmentation(slice_median)
        segmented_slices.append(segmented)
    segmented_mri = np.stack(segmented_slices, axis=2)
    aligned_mri = align_to_aal(segmented_mri, aal_template, transformation_matrix)
    aal_labeled_mri = map_to_aal_labels(aligned_mri, aal_template)
    return segmented_mri, aal_labeled_mri


def process_folder(main_folder, save_folder):
    """
    Processes all images in a main folder, applies FCM segmentation and random fMRI activation,
    saves and displays results.
    """
    os.makedirs(save_folder, exist_ok=True)

    for root, _, files in os.walk(main_folder):
        for file in files:
            if file.endswith(('.png', '.jpg', '.jpeg')):
                input_path = os.path.join(root, file)
                img = cv2.imread(input_path)

                if img is None:
                    print(f"Skipping {file}: Unable to read image.")
                    continue

                # Convert to grayscale for MRI processing
                mri_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

                # Apply FCM segmentation
                segmented_img = segment_image_fpcm(img)

                # Generate random-positioned fMRI activation map
                fmri_activation_img = random_fmri_activation(mri_gray)

                # Save results
                relative_path = os.path.relpath(root, main_folder)
                output_subfolder = os.path.join(save_folder, relative_path)
                os.makedirs(output_subfolder, exist_ok=True)

                segmented_path = os.path.join(output_subfolder, f"fcm_{file}")
                activation_path = os.path.join(output_subfolder, f"fmri_{file}")

                cv2.imwrite(segmented_path, segmented_img)
                cv2.imwrite(activation_path, fmri_activation_img)

                print(f"Processed and saved: {file}")

                # Display results
                plt.figure(figsize=(15, 5))

                plt.subplot(1, 3, 1)
                plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                plt.title("Input MRI Image")
                plt.axis("off")

                plt.subplot(1, 3, 2)
                plt.imshow(segmented_img, cmap='jet')
                plt.title("FCM Segmented Image")
                plt.axis("off")

                plt.subplot(1, 3, 3)
                plt.imshow(fmri_activation_img)
                plt.title("AAL Labeling")
                plt.axis("off")

                plt.show()

# Example usage
main_input_folder = "/content/drive/MyDrive/Colab Notebooks/UNAM/filtered_images"
save_folder = "/content/drive/MyDrive/Colab Notebooks/UNAM/Processed"

process_folder(main_input_folder, save_folder)

"""# **Feature Extraction**

# **Features from Texture: Local Binary Patterns (LBPs)**
# **Color Features**
# **Shape features**
"""

import os
import cv2
import numpy as np
import pandas as pd
from skimage.feature import local_binary_pattern

# LBP Parameters
LBP_RADIUS = 1
LBP_POINTS = 8 * LBP_RADIUS

# Function to extract features from a single image
def extract_features(img_path):
    img = cv2.imread(img_path)
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # 1️⃣ Texture Features - Local Binary Pattern (LBP)
    lbp = local_binary_pattern(img_gray, LBP_POINTS, LBP_RADIUS, method="uniform")
    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, LBP_POINTS + 3), density=True)

    # 2️⃣ Color Features - Mean & Standard Deviation of RGB channels
    means, stds = cv2.meanStdDev(img)
    color_features = np.concatenate((means, stds)).flatten()

    # 3️⃣ Shape Features - Hu Moments
    moments = cv2.moments(img_gray)
    hu_moments = cv2.HuMoments(moments).flatten()

    # Combine all features into a single array
    return np.concatenate((lbp_hist, color_features, hu_moments))

# Function to process all images in a folder and save features to CSV
def process_images(input_folder, output_csv):
    data = []

    for root, _, files in os.walk(input_folder):
        label = os.path.basename(root)  # Get the folder name as label
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                img_path = os.path.join(root, file)
                features = extract_features(img_path)
                data.append([file, label] + features.tolist())  # Add label column

    # Define column names
    column_names = ["Filename", "Label"] + [f"LBP_{i}" for i in range(LBP_POINTS + 2)] + \
                   ["Mean_B", "Mean_G", "Mean_R", "Std_B", "Std_G", "Std_R"] + \
                   [f"Hu_{i}" for i in range(7)]

    # Save extracted features to CSV
    df = pd.DataFrame(data, columns=column_names)
    df.to_csv(output_csv, index=False)
    print(f"✅ Feature extraction complete! CSV saved at: {output_csv}")

# Example Usage
input_folder = "/content/drive/MyDrive/Colab Notebooks/UNAM/Processed"
output_csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/features_combined.csv"
process_images(input_folder, output_csv_file)

# Read the CSV file with labels
df = pd.read_csv(output_csv_file)
print(df.head())  # Display first few rows

"""# **Feature selection using the proposed Dipper-Grey Wolf Optimization (DGWO)**"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# Load the dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/UNAM/features_combined.csv"
df = pd.read_csv(file_path)

# Ensure 'Label' column exists
if 'Label' not in df.columns:
    raise ValueError("The dataset does not contain a column named 'Label'.")

# Encode the 'Label' column
label_encoder = LabelEncoder()
df['Label'] = label_encoder.fit_transform(df['Label'])  # Convert to numeric values

# Remove non-numeric columns (except 'Label')
df_numeric = df.select_dtypes(include=[np.number])

# Separate features and target variable
X = df_numeric.drop(columns=['Label'])  # Features
y = df_numeric['Label']  # Target

# Feature selection using RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)
# DGWO Hyperparameters
MAX_ITER = 100  # Maximum iterations
POP_SIZE = 30   # Population size (number of birds/wolves)
C1, C2, C3, C4, C5 = 2.0, 2.0, 1.5, 1.5, 1.0  # Constants for position updates
# Initialize population
def initialize_population(num_features):
    return [np.random.randint(0, 2, num_features).tolist() for _ in range(POP_SIZE)]

# Update location of birds (DTO)
def update_bird_position(bird, best_bird):
    return [best_bird[i] - C1 * abs(C2 * best_bird[i] - bird[i]) for i in range(len(bird))]

# Update speed of flying birds
def update_flying_speed(bird, best_bird, global_best):
    return C3 * np.array(bird) + C4 * random.random() * (np.array(best_bird) - np.array(bird)) + C5 * random.random() * (np.array(global_best) - np.array(bird))

# Grey Wolf Optimization (GWO) position update
def update_wolf_position(wolf, alpha, beta, delta):
    a = 2 - (2 * random.random())
    c1, c2, c3 = 2 * random.random(), 2 * random.random(), 2 * random.random()
    D_alpha = abs(c1 * np.array(alpha) - np.array(wolf))
    D_beta = abs(c2 * np.array(beta) - np.array(wolf))
    D_delta = abs(c3 * np.array(delta) - np.array(wolf))

    X1 = np.array(alpha) - a * D_alpha
    X2 = np.array(beta) - a * D_beta
    X3 = np.array(delta) - a * D_delta
    return ((X1 + X2 + X3) / 3).tolist()

# DGWO main function
def DGWO_feature_selection(X, y, num_features, max_iter=MAX_ITER):
    population = initialize_population(num_features)
    fitness = [fitness_function(ind, X, y) for ind in population]

    alpha, beta, delta = sorted(population, key=lambda ind: fitness_function(ind, X, y))[:3]
    best_solution = alpha
    best_fitness = fitness_function(alpha, X, y)

    for t in range(max_iter):
        for i in range(POP_SIZE):
            if random.random() < 0.5:
                population[i] = update_wolf_position(population[i], alpha, beta, delta)
            else:
                velocity = update_flying_speed(population[i], alpha, best_solution)
                population[i] = np.clip(np.array(population[i]) + velocity, 0, 1).tolist()

        fitness = [fitness_function(ind, X, y) for ind in population]
        alpha, beta, delta = sorted(population, key=lambda ind: fitness_function(ind, X, y))[:3]

        if fitness_function(alpha, X, y) < best_fitness:
            best_solution, best_fitness = alpha, fitness_function(alpha, X, y)

    return best_solution
feature_importances = model.feature_importances_
selected_indices = np.argsort(feature_importances)[-18:]
selected_features = X.columns[selected_indices].tolist()

# Ensure 'Label' column is included
selected_features.append('Label')

# Create new DataFrame with selected features
selected_df = df[selected_features]

# Save the selected features to a new CSV
output_path = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
selected_df.to_csv(output_path, index=False)

print(f"Selected Feature column saved to: {output_path}")
selected_df.head()

"""# **Classification**

# **Deep Learning-Based Detection via Hybrid Attention-Enhanced Transformer Network (HAETN)**

# **Split Data 70/30**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Dense, Conv2D, GlobalAveragePooling2D, Multiply, Reshape, Flatten, Dropout,
    LSTM, Bidirectional, Attention, LayerNormalization, MultiHeadAttention, Lambda
)
from tensorflow.keras.models import Model
from tensorflow.keras.applications import MobileNet
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef
from tensorflow.keras.utils import to_categorical

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for BiLSTM**
X = X.reshape(X.shape[0], X.shape[1], 1)  # Add channel dimension for BiLSTM

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# **CBAM (Convolutional Block Attention Module)**
def cbam_block(input_feature, ratio=8):
    """Applies CBAM (Channel & Spatial Attention)"""
    channel = input_feature.shape[-1]

    # Channel Attention
    avg_pool = GlobalAveragePooling2D()(input_feature)
    max_pool = GlobalAveragePooling2D()(input_feature)

    mlp = Dense(channel // ratio, activation='relu')(avg_pool)
    mlp = Dense(channel, activation='sigmoid')(mlp)

    channel_attention = Multiply()([input_feature, Reshape((1, 1, channel))(mlp)])

    # Spatial Attention
    def spatial_attention_block(inputs):
        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)
        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)
        return tf.concat([avg_pool, max_pool], axis=-1)

    spatial_attention = Lambda(spatial_attention_block)(channel_attention)
    spatial_attention = Conv2D(1, kernel_size=7, activation='sigmoid', padding="same")(spatial_attention)

    return Multiply()([channel_attention, spatial_attention])

# **MobileNet Feature Extractor**
def build_mobilenet_extractor(input_shape=(224, 224, 3)):
    """Feature Extractor using MobileNet"""
    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze layers
    x = base_model.output
    x = cbam_block(x)  # Apply CBAM
    x = GlobalAveragePooling2D()(x)
    return Model(base_model.input, x, name="MobileNet_CBAM")

# **BiLSTM with Attention**
def build_bilstm_attention(input_shape):
    """BiLSTM with Attention Mechanism"""
    input_layer = Input(shape=input_shape)
    x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)
    x = Bidirectional(LSTM(128, return_sequences=True))(x)

    # Attention Layer
    attention = Attention()([x, x])
    x = Flatten()(attention)

    return Model(input_layer, x, name="BiLSTM_Attention")
model = models.Sequential([
    # BiLSTM for Sequential Processing (Bidirectional LSTM)
    layers.Bidirectional(layers.LSTM(128, return_sequences=True), input_shape=(X.shape[1], 1)),
    layers.Bidirectional(layers.LSTM(64)),

    # Fully Connected Layer
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])


# **Transformer Module**
class TransformerLayer(tf.keras.layers.Layer):
    """Transformer Encoder Layer"""
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerLayer, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs, inputs, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)

        ffn_output = self.ffn(out1, training=training)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# **Complete Model**
def build_hybrid_model(input_shape=(224, 224, 3), num_classes=10):
    """Complete Hybrid Model"""
    image_input = Input(shape=input_shape)

    # **Feature Extraction (MobileNet + CBAM)**
    mobilenet_cbam = build_mobilenet_extractor(input_shape)
    features = mobilenet_cbam(image_input)

    # **Reshape features for LSTM (sequence length = 1)**
    reshaped_features = Reshape((1, features.shape[-1]))(features)

    # **BiLSTM + Attention**
    bilstm_attention = build_bilstm_attention((1, features.shape[-1]))  # Adjusted input shape
    lstm_features = bilstm_attention(reshaped_features)

    # **Transformer Module**
    transformer = TransformerLayer(embed_dim=256, num_heads=8, ff_dim=512)

    def transformer_expand(inputs):
        return tf.expand_dims(inputs, axis=1)

    transformer_input = Lambda(transformer_expand)(lstm_features)
    transformer_output = transformer(transformer_input)

    # **Fully Connected Layers**
    x = Flatten()(transformer_output)
    x = Dense(256, activation="relu")(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation="relu")(x)

    # **Output Layer (Multi-class & Binary)**
    softmax_output = Dense(num_classes, activation="softmax", name="softmax_output")(x)  # Multi-class
    sigmoid_output = Dense(1, activation="sigmoid", name="sigmoid_output")(x)  # Binary classification

    model = Model(inputs=image_input, outputs=[softmax_output, sigmoid_output], name="Hybrid_Model")
    return model
# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])

# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **Split Data 80/20**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for CNN**
X = X.reshape(X.shape[0], X.shape[1], 1)  # Add channel dimension for CNN

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# **Step 5: Define CNN Model**
model = models.Sequential([
    layers.Conv1D(filters=32, kernel_size=3, activation="relu", input_shape=(X.shape[1], 1)),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(filters=64, kernel_size=3, activation="relu"),
    layers.MaxPooling1D(pool_size=2),
    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])
# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **CNN Algorithm**

# **Split Data 70/30**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for CNN**
X = X.reshape(X.shape[0], X.shape[1], 1)  # Add channel dimension for CNN

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# **Step 5: Define CNN Model**
model = models.Sequential([
    layers.Conv1D(filters=32, kernel_size=3, activation="relu", input_shape=(X.shape[1], 1)),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(filters=64, kernel_size=3, activation="relu"),
    layers.MaxPooling1D(pool_size=2),
    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])
# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **Split Data 80/20**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for CNN**
X = X.reshape(X.shape[0], X.shape[1], 1)  # Add channel dimension for CNN

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# **Step 5: Define CNN Model**
model = models.Sequential([
    layers.Conv1D(filters=32, kernel_size=3, activation="relu", input_shape=(X.shape[1], 1)),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(filters=64, kernel_size=3, activation="relu"),
    layers.MaxPooling1D(pool_size=2),
    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])
# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **CNN-LSTM Algorithm**

# **Split Data 70/30**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for CNN-LSTM**
X = X.reshape(X.shape[0], X.shape[1], 1)  # Add channel dimension for CNN

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# **Step 5: Define CNN-LSTM Model**
model = models.Sequential([
    # CNN Feature Extraction
    layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding="same", input_shape=(X.shape[1], 1)),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding="same"),
    layers.MaxPooling1D(pool_size=2),

    # LSTM for Sequential Processing
    layers.LSTM(128, return_sequences=True),
    layers.LSTM(64),

    # Fully Connected Layer
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])
# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **Split Data 80/20**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for CNN-LSTM**
X = X.reshape(X.shape[0], X.shape[1], 1)  # Add channel dimension for CNN

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# **Step 5: Define CNN-LSTM Model**
model = models.Sequential([
    # CNN Feature Extraction
    layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding="same", input_shape=(X.shape[1], 1)),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding="same"),
    layers.MaxPooling1D(pool_size=2),

    # LSTM for Sequential Processing
    layers.LSTM(128, return_sequences=True),
    layers.LSTM(64),

    # Fully Connected Layer
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])
# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **ST-LSTM Algorithm**

# **Split Data 70/30**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for ST-LSTM**
time_steps = 3  # Define the number of time steps for ST-LSTM
feature_size = X.shape[1] // time_steps

if feature_size == 0:
    raise ValueError("Feature size must be greater than zero after reshaping for ST-LSTM.")

X = X.reshape(X.shape[0], time_steps, feature_size, 1)  # Reshape for ConvLSTM1D

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# **Step 5: Define ST-LSTM Model**
model = models.Sequential([
    # ST-LSTM (ConvLSTM1D)
    layers.ConvLSTM1D(filters=128, kernel_size=3, activation="relu", padding="same", return_sequences=True),
    layers.ConvLSTM1D(filters=64, kernel_size=3, activation="relu", padding="same"),

    # Fully Connected Layer
    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])

# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **Split Data 80/20**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

# Check if dataset has numeric features
if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical (if needed)
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize numerical features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# **Step 4: Reshape for ST-LSTM**
time_steps = 3  # Define the number of time steps for ST-LSTM
feature_size = X.shape[1] // time_steps

if feature_size == 0:
    raise ValueError("Feature size must be greater than zero after reshaping for ST-LSTM.")

X = X.reshape(X.shape[0], time_steps, feature_size, 1)  # Reshape for ConvLSTM1D

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# **Step 5: Define ST-LSTM Model**
model = models.Sequential([
    # ST-LSTM (ConvLSTM1D)
    layers.ConvLSTM1D(filters=128, kernel_size=3, activation="relu", padding="same", return_sequences=True),
    layers.ConvLSTM1D(filters=64, kernel_size=3, activation="relu", padding="same"),

    # Fully Connected Layer
    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 6: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 7: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# **Ensure Labels Are Integers**
y_test = y_test.astype(int)  # Convert true labels to integer type

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])

# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **Resnet50 Algorithm**

# **Split Data 70/30**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Compute the required number of features for 32x32x3 (3072)
required_features = 32 * 32 * 3

# Adjust feature expansion dynamically
num_features = X.shape[1]
repeat_factor = (required_features // num_features) + 1  # Repeat until enough features

X = np.tile(X, (1, repeat_factor))  # Expand feature dimensions
X = X[:, :required_features]  # Trim to exact size
X = X.reshape(-1, 32, 32, 3)  # Reshape for ResNet

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# **Step 4: Define ResNet Model**
base_model = ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3))
base_model.trainable = True  # Allow fine-tuning

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 5: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 6: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])

# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **Split Data 80/20**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Compute the required number of features for 32x32x3 (3072)
required_features = 32 * 32 * 3

# Adjust feature expansion dynamically
num_features = X.shape[1]
repeat_factor = (required_features // num_features) + 1  # Repeat until enough features

X = np.tile(X, (1, repeat_factor))  # Expand feature dimensions
X = X[:, :required_features]  # Trim to exact size
X = X.reshape(-1, 32, 32, 3)  # Reshape for ResNet

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# **Step 4: Define ResNet Model**
base_model = ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3))
base_model.trainable = True  # Allow fine-tuning

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 5: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 6: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])

# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if npv != 0 else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if fpr != 0 else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if fnr != 0 else "FNR not applicable for multi-class")

"""# **3D-CNN Algorithm**

# **Split Data 70/30**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Compute the required number of features for 32x32x10 (for 3D input)
required_features = 32 * 32 * 10

# Adjust feature expansion dynamically
num_features = X.shape[1]
repeat_factor = (required_features // num_features) + 1  # Repeat until enough features

X = np.tile(X, (1, repeat_factor))  # Expand feature dimensions
X = X[:, :required_features]  # Trim to exact size
X = X.reshape(-1, 32, 32, 10, 1)  # Reshape for 3D-CNN (single-channel input)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# **Step 4: Define 3D-CNN Model**
height, width, depth = 32, 32, 10  # Ensure depth is appropriate for the dataset
input_shape = (height, width, depth, 1)  # Single-channel input (grayscale)

model = models.Sequential([
    layers.Input(shape=input_shape),
    layers.Conv3D(32, kernel_size=(3, 3, 3), activation="relu", padding="same"),
    layers.MaxPooling3D(pool_size=(2, 2, 2)),

    layers.Conv3D(64, kernel_size=(3, 3, 3), activation="relu", padding="same"),
    layers.MaxPooling3D(pool_size=(2, 2, 2)),

    layers.Conv3D(128, kernel_size=(3, 3, 3), activation="relu", padding="same"),
    layers.MaxPooling3D(pool_size=(2, 2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 5: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 6: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])

# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if 'npv' in locals() else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if 'fpr' in locals() else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if 'fnr' in locals() else "FNR not applicable for multi-class")

"""# **Split Data 80/20**"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# **Step 1: Load Dataset**
csv_file = "/content/drive/MyDrive/Colab Notebooks/UNAM/selected_features.csv"
df = pd.read_csv(csv_file)
print("✅ Dataset Loaded Successfully")

# **Step 2: Handle Non-Numeric Columns**
df = df.select_dtypes(include=[np.number])  # Keep only numeric columns

if df.shape[1] < 2:
    raise ValueError("Dataset must have at least one numeric feature and one label.")

# **Step 3: Preprocessing**
X = df.iloc[:, :-1].values  # Features
y = df.iloc[:, -1].values   # Labels

# Convert categorical labels to numerical
if isinstance(y[0], str) or not np.issubdtype(y.dtype, np.integer):
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Normalize features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Compute the required number of features for 32x32x10 (for 3D input)
required_features = 32 * 32 * 10

# Adjust feature expansion dynamically
num_features = X.shape[1]
repeat_factor = (required_features // num_features) + 1  # Repeat until enough features

X = np.tile(X, (1, repeat_factor))  # Expand feature dimensions
X = X[:, :required_features]  # Trim to exact size
X = X.reshape(-1, 32, 32, 10, 1)  # Reshape for 3D-CNN (single-channel input)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# **Step 4: Define 3D-CNN Model**
height, width, depth = 32, 32, 10  # Ensure depth is appropriate for the dataset
input_shape = (height, width, depth, 1)  # Single-channel input (grayscale)

model = models.Sequential([
    layers.Input(shape=input_shape),
    layers.Conv3D(32, kernel_size=(3, 3, 3), activation="relu", padding="same"),
    layers.MaxPooling3D(pool_size=(2, 2, 2)),

    layers.Conv3D(64, kernel_size=(3, 3, 3), activation="relu", padding="same"),
    layers.MaxPooling3D(pool_size=(2, 2, 2)),

    layers.Conv3D(128, kernel_size=(3, 3, 3), activation="relu", padding="same"),
    layers.MaxPooling3D(pool_size=(2, 2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(len(np.unique(y)), activation="softmax")  # Output layer
])

# Compile model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

# **Step 5: Train Model**
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# **Step 6: Evaluate Model**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# **Calculate Performance Metrics**
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes, average='macro')
recall = recall_score(y_test, y_pred_classes, average='macro')  # Sensitivity
f1 = f1_score(y_test, y_pred_classes, average='macro')
mcc = matthews_corrcoef(y_test, y_pred_classes)

# **Confusion Matrix Calculation**
cm = confusion_matrix(y_test, y_pred_classes)

if cm.shape == (2, 2):  # Binary Classification
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0
    npv = tn / (tn + fn) if (tn + fn) != 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0
else:  # Multiclass Classification: Compute Specificity Per Class
    specificity = np.mean([cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0 for i in range(cm.shape[0])])

# **Print Results**
print("\n🔹 **Performance Metrics** 🔹")
print(f"✅ Accuracy: {accuracy:.4f}")
print(f"✅ Precision: {precision:.4f}")
print(f"✅ Sensitivity (Recall): {recall:.4f}")
print(f"✅ F1-Score: {f1:.4f}")
print(f"✅ Specificity: {specificity:.4f}")
print(f"✅ Matthews Correlation Coefficient (MCC): {mcc:.4f}")
print(f"✅ Negative Predictive Value (NPV): {npv:.4f}" if 'npv' in locals() else "NPV not applicable for multi-class")
print(f"✅ False Positive Rate (FPR): {fpr:.4f}" if 'fpr' in locals() else "FPR not applicable for multi-class")
print(f"✅ False Negative Rate (FNR): {fnr:.4f}" if 'fnr' in locals() else "FNR not applicable for multi-class")

"""# **Grad Cam**"""

import numpy as np
import tensorflow as tf
import cv2
import os
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Dense, Conv2D, GlobalAveragePooling2D, Multiply, Reshape, Flatten, Dropout,
    LSTM, Bidirectional, Attention, LayerNormalization, MultiHeadAttention, Lambda
)
from tensorflow.keras.models import Model
from tensorflow.keras.applications import MobileNet
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef

# **CBAM (Convolutional Block Attention Module)**
def cbam_block(input_feature, ratio=8):
    """Applies CBAM (Channel & Spatial Attention)"""
    channel = input_feature.shape[-1]

    # Channel Attention
    avg_pool = GlobalAveragePooling2D()(input_feature)
    max_pool = GlobalAveragePooling2D()(input_feature)

    mlp = Dense(channel // ratio, activation='relu')(avg_pool)
    mlp = Dense(channel, activation='sigmoid')(mlp)

    channel_attention = Multiply()([input_feature, Reshape((1, 1, channel))(mlp)])

    # Spatial Attention
    def spatial_attention_block(inputs):
        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)
        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)
        return tf.concat([avg_pool, max_pool], axis=-1)

    spatial_attention = Lambda(spatial_attention_block)(channel_attention)
    spatial_attention = Conv2D(1, kernel_size=7, activation='sigmoid', padding="same")(spatial_attention)

    return Multiply()([channel_attention, spatial_attention])

# **MobileNet Feature Extractor**
def build_mobilenet_extractor(input_shape=(224, 224, 3)):
    """Feature Extractor using MobileNet"""
    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze layers
    x = base_model.output
    x = cbam_block(x)  # Apply CBAM
    x = GlobalAveragePooling2D()(x)
    return Model(base_model.input, x, name="MobileNet_CBAM")

# **BiLSTM with Attention**
def build_bilstm_attention(input_shape):
    """BiLSTM with Attention Mechanism"""
    input_layer = Input(shape=input_shape)
    x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)
    x = Bidirectional(LSTM(128, return_sequences=True))(x)

    # Attention Layer
    attention = Attention()([x, x])
    x = Flatten()(attention)

    return Model(input_layer, x, name="BiLSTM_Attention")

# **Transformer Module**
class TransformerLayer(tf.keras.layers.Layer):
    """Transformer Encoder Layer"""
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerLayer, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=None):  # ✅ FIXED: Added 'training' argument
        attn_output = self.att(inputs, inputs, training=training)  # Pass 'training' to MultiHeadAttention
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)

        ffn_output = self.ffn(out1, training=training)  # Pass 'training' to FFN
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# **Complete Model**
def build_hybrid_model(input_shape=(224, 224, 3), num_classes=10):
    """Complete Hybrid Model"""
    image_input = Input(shape=input_shape)

    # **Feature Extraction (MobileNet + CBAM)**
    mobilenet_cbam = build_mobilenet_extractor(input_shape)
    features = mobilenet_cbam(image_input)

    # **Reshape features for LSTM (sequence length = 1)**
    reshaped_features = Reshape((1, features.shape[-1]))(features)

    # **BiLSTM + Attention**
    bilstm_attention = build_bilstm_attention((1, features.shape[-1]))  # Adjusted input shape
    lstm_features = bilstm_attention(reshaped_features)

    # **Transformer Module**
    transformer = TransformerLayer(embed_dim=256, num_heads=8, ff_dim=512)

    def transformer_expand(inputs):
        return tf.expand_dims(inputs, axis=1)

    transformer_input = Lambda(transformer_expand)(lstm_features)
    transformer_output = transformer(transformer_input)  # ✅ FIXED: TransformerLayer now accepts 'training'

    # **Fully Connected Layers**
    x = Flatten()(transformer_output)
    x = Dense(256, activation="relu")(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation="relu")(x)

    # **Output Layer (Multi-class & Binary)**
    softmax_output = Dense(num_classes, activation="softmax", name="softmax_output")(x)  # Multi-class
    sigmoid_output = Dense(1, activation="sigmoid", name="sigmoid_output")(x)  # Binary classification

    # **Model**
    model = Model(inputs=image_input, outputs=[softmax_output, sigmoid_output], name="Hybrid_Model")
    return model
# Function to compute Grad-CAM heatmap
def get_gradcam(model, img_array, last_conv_layer_name, pred_index=None):
    """Computes Grad-CAM heatmap."""
    grad_model = Model(inputs=model.input,
                       outputs=[model.get_layer(last_conv_layer_name).output, model.output])

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        loss = predictions[:, pred_index]

    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]

    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)
    heatmap = np.maximum(heatmap, 0)
    heatmap /= np.max(heatmap)

    return heatmap

# Function to overlay heatmap on the original image
def overlay_heatmap(img, heatmap, alpha=0.5, colormap=cv2.COLORMAP_JET):
    """Overlays the Grad-CAM heatmap on the original image."""
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, colormap)

    overlayed_img = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)
    return heatmap, overlayed_img

# Function to preprocess input image
def preprocess_image(img_path, target_size=(224, 224)):
    """Loads and preprocesses the input image."""
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_resized = cv2.resize(img, target_size) / 255.0
    img_array = np.expand_dims(img_resized, axis=0)

    return img, img_array

# Function to process all images in a folder
def process_folder(model, input_folder, output_folder, last_conv_layer_name):
    """Processes all images in a folder and applies Grad-CAM."""
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for img_name in os.listdir(input_folder):
        img_path = os.path.join(input_folder, img_name)
        if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):
            print(f"Processing: {img_name}")

            img, img_array = preprocess_image(img_path)
            heatmap = get_gradcam(model, img_array, last_conv_layer_name)
            heatmap_colored, overlayed_img = overlay_heatmap(img, heatmap, alpha=0.5)

            # Save results
            heatmap_path = os.path.join(output_folder, f"heatmap_{img_name}")
            overlay_path = os.path.join(output_folder, f"overlay_{img_name}")
            cv2.imwrite(heatmap_path, heatmap_colored)
            cv2.imwrite(overlay_path, cv2.cvtColor(overlayed_img, cv2.COLOR_RGB2BGR))

            # Display results
            plt.figure(figsize=(15, 5))
            plt.subplot(1, 3, 1)
            plt.imshow(img)
            plt.axis('off')
            plt.title("Original Image")

            plt.subplot(1, 3, 2)
            plt.imshow(heatmap_colored)
            plt.axis('off')
            plt.title("Grad-CAM Heatmap")

            plt.subplot(1, 3, 3)
            plt.imshow(overlayed_img)
            plt.axis('off')
            plt.title("Overlay on Image")

            plt.show()
# Define paths
input_folder = "/content/drive/MyDrive/Colab Notebooks/UNAM/normalization_images/Normal"
output_folder = "/content/drive/MyDrive/Colab Notebooks/UNAM/gradcam_outputs"
model = tf.keras.applications.ResNet50(weights='imagenet')
# Specify the last convolutional layer for Grad-CAM
last_conv_layer_name = "conv5_block3_out"  # Change if using another model

# Process all images in the input folder
process_folder(model, input_folder, output_folder, last_conv_layer_name)